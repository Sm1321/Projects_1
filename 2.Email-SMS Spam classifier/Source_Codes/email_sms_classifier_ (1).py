# -*- coding: utf-8 -*-
"""Email /SMS Classifier .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11o9Pg-e5DtaX-skqGGvLFge61xChnToW
"""

import pandas as pd
import numpy as np

df = pd.read_csv('/content/spam.tsv',sep='\t')

d= pd.read_csv('/content/spam.tsv',sep='\t')
d.head()

df = df[['label','message']]

df.head()

df.shape

"""### **Steps to be Followed:**

- 1. Data cleaning
- 2. EDA
- 3. Text Preprocessing
- 4. Model building
- 5. Evaluation
- 6. Deploy into Stremlit Web APP

### **1. Data Cleaning**
"""

df.info()

df.head()

# renaming the cols
df.rename(columns={'label':'target','message':'text'},inplace=True)
df.sample(5)

##Missing values
df.isnull().sum()

##Check for the duplicates
df.duplicated().sum()

##Remove the duplicates
df = df.drop_duplicates(keep='first')
#This removes duplicate rows from the DataFrame,
#keeping only the first occurrence of each duplicate row.

##Check for the duplicates
df.duplicated().sum()

df.shape

"""### **2.EDA**"""

df.head()

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
df['target'] = encoder.fit_transform(df['target'])

df['target'].value_counts()

#ham : 0
#spam :1

import matplotlib.pyplot as plt
plt.pie(df['target'].value_counts(), labels=['ham','spam'],autopct="%0.2f")
plt.show()

##Data is Imbalanced

import nltk
nltk.download('punkt')

#length of number of characters
df['num_characters'] = df['text'].apply(len)

df.head()

ans = nltk.word_tokenize(df['text'][0]) ### it will seperate all the words and punctuations
print(ans)

## number of words in the text and find it's length
df['num_words'] = df['text'].apply(lambda x:len(nltk.word_tokenize(x)))

df.head()

## seperate the senteces ,when pullstop is there
ans_1 = nltk.sent_tokenize(df['text'][0])
print(ans_1)

#find the numner of seneteces present in the text
df['num_sentences'] = df['text'].apply(lambda x:len(nltk.sent_tokenize(x)))

df.head()

df[['num_characters','num_words','num_sentences']].describe()

##ham
df[df['target']==0][['num_characters','num_words','num_sentences']].describe()

##spam
df[df['target']==1][['num_characters','num_words','num_sentences']].describe()

import seaborn as sns

plt.figure(figsize = (12,6))
sns.histplot(df[df['target']=='ham']['num_characters'])
sns.histplot(df[df['target']=='spam']['num_characters'])

sns.histplot(df[df['target']==0]['num_words'])
sns.histplot(df[df['target']==1]['num_words'])

sns.pairplot(df,hue ='target')

df.head()

"""### **3.Data PreProcessing**
- Lower case
- Tokenization
- Removing special characters
- Removing stop words and punctuation
- Stemming

"""

#Lower case
#split the all words

def transform_text_1(text):
  text = text.lower()
  text = nltk.word_tokenize(text)#it iwll break the words ,in the sentence by space,punct,p
  return text

print(transform_text_1(df['text'][0]))

print(transform_text_1('Hi How are you? what!s up.'))

"""### **text -preprocessing**"""

#Lower case
#split the all words
#remove the special characters


def transform_text_1(text):
  text = text.lower()
  text = nltk.word_tokenize(text)#it iwll break the words ,in the sentence by space,punct,comma

  y = []
  for i in text:
    if i.isalnum():#if it is alphanum
      y.append(i)

  return y

df['text'][200]

print(transform_text_1('Hi, How are you? what!s up.'))

print(transform_text_1(df['text'][200]))

import nltk
nltk.download('stopwords')
import string # Import the string module

from nltk.corpus import stopwords
print(stopwords.words('english'))

#Lower case
#split the all words
#remove the special characters
#remove the Stopwords and Punctions


def transform_text_2(text):
  text = text.lower()
  text = nltk.word_tokenize(text)#it iwll break the words ,in the sentence by space,punct,comma

  y = []
  for i in text:
    if i.isalnum():#if it is alphanum
      y.append(i)

  text = y[:] # copy to text
  y.clear() #clear the list

  for i in text :
    if i not in stopwords.words('english') and i not in string.punctuation:
      y.append(i)

  return y

#['found', 'it', 'enc', 'lt', 'gt', 'where', 'you', 'at'] - Previous ttransform_text_1 output
transform_text_2(df['text'][200])

print(transform_text_2('Hi, How are you? what!s up.'))

from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()
ps.stem('dancing')

#Lower case
#split the all words
#remove the special characters
#remove the Stopwords and Punctions
#apply the stemming

#####################   This is the Main Function #######################
def transform_text(text):
  text = text.lower()
  text = nltk.word_tokenize(text)#it iwll break the words ,in the sentence by space,punct,comma

  y = []
  for i in text:
    if i.isalnum():#if it is alphanum
      y.append(i)

  text = y[:] # copy to text
  y.clear() #clear the list

  for i in text :
    if i not in stopwords.words('english') and i not in string.punctuation:
      y.append(i)


  text = y[:] #copy the text
  y.clear() #clear it

  for i in text:
    y.append(ps.stem(i))

  return " ".join(y)

# ['found', 'enc', 'lt', 'gt'] #prev output
transform_text(df['text'][200])

print(transform_text(""" i am really enjoying the Natural Processing Language Classes and its Implemnataion! It's Amazing """))

df['transformed_text']= df['text'].apply(transform_text)

df.head()

from wordcloud import WordCloud
wc = WordCloud(width=500,height=500,min_font_size=10,background_color='white')

"""
**The WordCloud class from the wordcloud library in Python is used to generate word clouds, which are visual representations of text data where the size of each word indicates its frequency or importance in the dataset.**"""

spam_wc = wc.generate(df[df['target'] == 1]['transformed_text'].str.cat(sep=" "))

plt.figure(figsize=(15,6))
plt.imshow(spam_wc)

ham_wc = wc.generate(df[df['target'] == 0]['transformed_text'].str.cat(sep=" "))

plt.figure(figsize=(15,6))
plt.imshow(ham_wc)

df.head()

df[df['target']==1]['transformed_text'].tolist()

##split all the words and store in the list
spam_corpus = []
for msg in df[df['target']==1]['transformed_text'].tolist():
  for word in msg.split():
    spam_corpus.append(word)

len(spam_corpus)

from collections import Counter
Counter(spam_corpus).most_common(30)

sns.barplot(x=pd.DataFrame(Counter(spam_corpus).most_common(30))[0],y=pd.DataFrame(Counter(spam_corpus).most_common(30))[1])
plt.xticks(rotation='vertical')
plt.title("Most Common Repated 30 words in the Spam in Target Colums")
plt.show()

ham_corpus = []
for msg in df[df['target']==0]['transformed_text'].tolist():
  for word in msg.split():
    ham_corpus.append(word)

len(ham_corpus)

from collections import Counter
Counter(ham_corpus).most_common(30)

##Now Plot the Most Coomon 30 words of the Ham  in Target column

sns.barplot(x=pd.DataFrame(Counter(ham_corpus).most_common(30))[0],y=pd.DataFrame(Counter(ham_corpus).most_common(30))[1])
plt.xticks(rotation='vertical')
plt.title("Most Common Repated 30 words in the Ham in Target Colums")
plt.show()

df.head()

df.shape

"""### **Model Building**"""



from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
cv = CountVectorizer()
tfidf = TfidfVectorizer(max_features = 3000)

X = tfidf.fit_transform(df['transformed_text']).toarray()

X.shape

X

y = df['target'].values

y

"""**TRain-Test-Split**"""

from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state= 2)

from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB
from sklearn.metrics import accuracy_score,confusion_matrix,precision_score

gnb = GaussianNB()
mnb = MultinomialNB()
bnb = BernoulliNB()

gnb.fit(X_train,y_train)
y_pred_1 = gnb.predict(X_test)
print("Accuracy Score :-")
print(accuracy_score(y_test,y_pred_1))
print("\nConfusion Matrix:-")
print(confusion_matrix(y_test,y_pred_1))
print("\nPrecision Score:-")
print(precision_score(y_test,y_pred_1))

mnb.fit(X_train,y_train)
y_pred_2 = mnb.predict(X_test)
print("Accuracy Score :-")
print(accuracy_score(y_test,y_pred_2))
print("\nConfusion Matrix:-")
print(confusion_matrix(y_test,y_pred_2))
print("\nPrecision Score:-")
print(precision_score(y_test,y_pred_2))

bnb.fit(X_train,y_train)
y_pred_3 = bnb.predict(X_test)
print("Accuracy Score :-")
print(accuracy_score(y_test,y_pred_3))

print("\nConfusion Matrix:-")
print(confusion_matrix(y_test,y_pred_3))

print("\nPrecision Score:-")
print(precision_score(y_test,y_pred_3))

## TFidf -> MNB

## USe the all classifiers
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
#from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import BaggingClassifier

svc = SVC(kernel = 'sigmoid',gamma = 1.0)
knc = KNeighborsClassifier()
lrc = LogisticRegression()
rfc = RandomForestClassifier(n_estimators = 50,random_state =2)
mnb = MultinomialNB()
dtc = DecisionTreeClassifier(max_depth = 5)
abc = AdaBoostClassifier(n_estimators = 50,random_state =2)
bc = BaggingClassifier(n_estimators = 50,random_state =2)
gbc = GradientBoostingClassifier(n_estimators = 50,random_state =2)
#etc = ExtraTreesClassifier()
xgb = XGBClassifier(n_estimators = 50,random_state =2)

clfs = {
    'SVC' : svc,
    'KNN':knc,
    'LogisticRegression':lrc,
    'RandomForestClassifier':rfc,
    'MultinomialNB':mnb,
    'DecisionTreeClassifier':dtc,
    'AdaBoostClassifier':abc,
    'BaggingClassifier':bc,
    'GradientBoostingClassifier':gbc,
    'XGBClassifier':xgb


}

def train_classifer(clfs,X_train,y_train,X_test,y_test):
  clfs.fit(X_train,y_train)
  y_pred = clfs.predict(X_test)
  accuracy = accuracy_score(y_test,y_pred)


  return accuracy

train_classifer(svc,X_train,y_train,X_test,y_test)

all_accuracy_scores = []




for name,clf in clfs.items():
  current_accuracy = train_classifer(clf,X_train,y_train,X_test,y_test)
  print(name,"Model:- ")
  print("Accuracy :-",current_accuracy)
  all_accuracy_scores.append(current_accuracy)
  print("----"*20)

df_of_all_scores = pd.DataFrame({'Alogorithm Used':clfs.keys(),'Accuracy':all_accuracy_scores}).sort_values('Accuracy',ascending=False)

df_of_all_scores

##SVC and Multinomial Naive Bayes give More Accurcy here


#we, use the multinomial Naive Bayes

from sklearn.model_selection import GridSearchCV
from sklearn.naive_bayes import MultinomialNB

mnb_params = {
    'alpha': [0.5, 1.0, 1.5, 2.0]
}

mnb_grid = GridSearchCV(MultinomialNB(), mnb_params, cv=5, scoring='accuracy', n_jobs=-1)
mnb_grid.fit(X_train, y_train)

print(f"Best parameters for Multinomial Naive Bayes: {mnb_grid.best_params_}")
print(f"Best cross-validation accuracy for Multinomial Naive Bayes: {mnb_grid.best_score_}")

# Retrain the best model on the entire training data
best_mnb = mnb_grid.best_estimator_

best_mnb

best_mnb.fit(X_train, y_train)

import pickle

# Save the model
with open('mnb_model.pkl', 'wb') as model_file:
    pickle.dump(best_mnb, model_file)

# Save the TF-IDF vectorizer
with open('tfidf_vectorizer.pkl', 'wb') as vectorizer_file:
    pickle.dump(tfidf, vectorizer_file)

# Load the model and TF-IDF vectorizer
with open('mnb_model.pkl', 'rb') as model_file:
    model = pickle.load(model_file)

with open('tfidf_vectorizer.pkl', 'rb') as vectorizer_file:
    tfidf_vectorizer = pickle.load(vectorizer_file)

##input message

input_message = ["WELL DONE! Your 4* Costa Del Sol Holiday or £5000 await collection. Call 09050090044 Now toClaim. SAE, TCs, POBox334, Stockport, SK38xh, Cost£1.50/pm, Max10mins"]


##fit the tfidf
input_message = tfidf_vectorizer.transform(input_message)


#predict
prediction = model.predict(input_message)


if prediction[0]==1:
  print("Spam")
else:
  print("Ham")



